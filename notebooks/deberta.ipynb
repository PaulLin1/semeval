{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJc6NGCbScUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108a5911-53c8-4c58-d6e8-b3ed426758f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.2.3 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.57.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q numpy pandas transformers torch scikit-learn nltk googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y1Y4AKpYw89"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UFdNG3cHt91C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVFOVt7q1pMt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he2v5QmBSsWg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('good_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation = df[2550:].copy()\n",
        "df = df[:2550]"
      ],
      "metadata": {
        "id": "wNU1YCpo15fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-GXsJcqU-rr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f5714d-81fe-4248-dcb8-aaf1605f6bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# from transformers import DebertaTokenizer, DebertaModel\n",
        "# tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from nltk.corpus import wordnet\n",
        "from googletrans import Translator\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 1. Synonym Replacement\n",
        "def synonym_replacement(sentence, n=1):\n",
        "    words = sentence.split()\n",
        "    new_words = words.copy()\n",
        "\n",
        "    for  _ in range(n):\n",
        "        word = random.choice(words)\n",
        "        synonyms = set()\n",
        "\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.add(lemma.name())\n",
        "\n",
        "        if synonyms:\n",
        "            synonym = random.choice(list(synonyms))\n",
        "            new_words = [synonym if w == word else w for w in new_words]\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# 2. Random Insertion\n",
        "def random_insertion(sentence, n=1):\n",
        "    words = sentence.split()\n",
        "    new_words = words.copy()\n",
        "\n",
        "    for _ in range(n):\n",
        "        word = random.choice(words)\n",
        "        synonyms = set()\n",
        "\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.add(lemma.name())\n",
        "\n",
        "        if synonyms:\n",
        "            synonym = random.choice(list(synonyms))\n",
        "            insert_position = random.randint(0, len(new_words) - 1)\n",
        "            new_words.insert(insert_position, synonym)\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# 3. Random Swap\n",
        "def random_swap(sentence, n=1):\n",
        "    words = sentence.split()\n",
        "    new_words = words.copy()\n",
        "\n",
        "    for _ in range(n):\n",
        "        idx1 = random.randint(0, len(words) - 1)\n",
        "        idx2 = random.randint(0, len(words) - 1)\n",
        "\n",
        "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# 4. Random Deletion\n",
        "def random_deletion(sentence, p=0.2):\n",
        "    words = sentence.split()\n",
        "    if len(words) == 1:  # We cannot delete the only word in the sentence\n",
        "        return sentence\n",
        "\n",
        "    new_words = [word for word in words if random.uniform(0, 1) > p]\n",
        "\n",
        "    if len(new_words) == 0:\n",
        "        return random.choice(words)  # Return a random word if all are deleted\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# 5. Back Translation (Translation to another language and then back to the original)\n",
        "def back_translation(sentence, src_lang='en', tgt_lang='fr'):\n",
        "    translator = Translator()\n",
        "    translated = translator.translate(sentence, src=src_lang, dest=tgt_lang).text\n",
        "    back_translated = translator.translate(translated, src=tgt_lang, dest=src_lang).text\n",
        "    return back_translated\n",
        "\n",
        "def EDA(sentence):\n",
        "    sentence = synonym_replacement(sentence, n=2)\n",
        "    sentence = random_insertion(sentence, n=2)\n",
        "    sentence = random_swap(sentence, n=2)\n",
        "    sentence = random_deletion(sentence, p=0.2)\n",
        "    # sentence = back_translation(sentence, src_lang='en', tgt_lang='fr')\n",
        "\n",
        "    return sentence\n",
        "\n",
        "def EDA2(sentence):\n",
        "    sentence = synonym_replacement(sentence, n=3)\n",
        "    sentence = random_insertion(sentence, n=3)\n",
        "    sentence = random_swap(sentence, n=3)\n",
        "    sentence = random_deletion(sentence, p=0.3)\n",
        "    # sentence = back_translation(sentence, src_lang='en', tgt_lang='fr')\n",
        "\n",
        "    return sentence\n",
        "\n",
        "def EDA3(sentence):\n",
        "    sentence = synonym_replacement(sentence, n=1)\n",
        "    sentence = random_insertion(sentence, n=1)\n",
        "    sentence = random_swap(sentence, n=1)\n",
        "    sentence = random_deletion(sentence, p=0.1)\n",
        "    # sentence = back_translation(sentence, src_lang='en', tgt_lang='fr')\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF5oa1bzVp8r",
        "outputId": "8005b2a9-508f-40b3-9241-ca156a1ddeed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MquomP5U_Gf"
      },
      "outputs": [],
      "source": [
        "z1 = df.copy()\n",
        "z1['text'] = z1['text'].apply(EDA)\n",
        "\n",
        "df = pd.concat([df, z1], ignore_index=True)\n",
        "\n",
        "encoded = df['text'].apply(lambda x: tokenizer.encode_plus(\n",
        "    x,\n",
        "    max_length=64,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True\n",
        "))\n",
        "\n",
        "\n",
        "df['input_ids'] = encoded.apply(lambda x: x['input_ids'])\n",
        "df['attention_mask'] = encoded.apply(lambda x: x['attention_mask'])\n",
        "\n",
        "\n",
        "\n",
        "encoded = validation['text'].apply(lambda x: tokenizer.encode_plus(\n",
        "    x,\n",
        "    max_length=64,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True\n",
        "))\n",
        "\n",
        "\n",
        "validation['input_ids'] = encoded.apply(lambda x: x['input_ids'])\n",
        "validation['attention_mask'] = encoded.apply(lambda x: x['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "go_emo = pd.read_csv('1.csv')\n",
        "non_zero_emotions = go_emo[['text', 'anger', 'fear', 'joy', 'sadness', 'surprise']]\n",
        "\n",
        "non_zero_emotions = non_zero_emotions[\n",
        "    (go_emo['anger'] != 0) |\n",
        "    (go_emo['fear'] != 0) |\n",
        "    (go_emo['joy'] != 0) |\n",
        "    (go_emo['sadness'] != 0) |\n",
        "    (go_emo['surprise'] != 0)\n",
        "]\n",
        "\n",
        "\n",
        "# Get 500 random rows where all emotions are zero\n",
        "zero_emotions = go_emo[\n",
        "    (go_emo['anger'] == 0) &\n",
        "    (go_emo['fear'] == 0) &\n",
        "    (go_emo['joy'] == 0) &\n",
        "    (go_emo['sadness'] == 0) &\n",
        "    (go_emo['surprise'] == 0)\n",
        "].sample(n=150, random_state=42)\n",
        "\n",
        "zero_emotions = zero_emotions[['text', 'anger', 'fear', 'joy', 'sadness', 'surprise']]\n",
        "\n",
        "# Concatenate the two dataframes\n",
        "go_emo = pd.concat([non_zero_emotions, zero_emotions])"
      ],
      "metadata": {
        "id": "9hLCAPCUIDFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = go_emo['text'].apply(lambda x: tokenizer.encode_plus(\n",
        "    x,\n",
        "    max_length=64,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True\n",
        "))\n",
        "\n",
        "\n",
        "go_emo['input_ids'] = encoded.apply(lambda x: x['input_ids'])\n",
        "go_emo['attention_mask'] = encoded.apply(lambda x: x['attention_mask'])"
      ],
      "metadata": {
        "id": "zCojR5PLIsnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4rfHZKzduqZ"
      },
      "outputs": [],
      "source": [
        "emotions = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
        "emotions2 = ['anger', 'fear', 'joy', 'sadness', 'surprise']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilMRwiOMfNMf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, labels):\n",
        "        self.df = df.copy()\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx, :]\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(row['input_ids']),\n",
        "            'attention_mask': torch.tensor(row['attention_mask']),\n",
        "            'labels': torch.tensor(row[self.labels].tolist(), dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "train_dataset = TweetsDataset(df, emotions2)\n",
        "val_dataset = TweetsDataset(validation, emotions2)\n",
        "go_emo_dataset = TweetsDataset(go_emo, emotions2)\n",
        "\n",
        "# train_size = int(len(df) * .8)\n",
        "# val_size = len(df) - train_size\n",
        "\n",
        "# train_dataset, val_dataset = random_split(\n",
        "#     dataset,\n",
        "#     [train_size, val_size],\n",
        "#     generator=torch.Generator().manual_seed(42)\n",
        "# )\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
        "go_dataloader = DataLoader(go_emo_dataset, batch_size=16)\n",
        "\n",
        "# full_dataloader = DataLoader(train_dataset, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veb5rsVtYZwV"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBBMic0OgCw9"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TweetEmotionClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "        self.layer = nn.LayerNorm(768)\n",
        "        self.linear = nn.Linear(768, 5)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # h = self.layer(cls_output)\n",
        "        h = self.dropout(cls_output)\n",
        "        h = self.linear(h)\n",
        "        # h = F.relu(h)\n",
        "\n",
        "        # h = self.dropout(h)\n",
        "        # out = self.linear2(h)\n",
        "\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1sAA0dejXDU"
      },
      "outputs": [],
      "source": [
        "model = TweetEmotionClassifier()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.bert.parameters(), 'lr': 2e-5},\n",
        "    {'params': list(model.linear.parameters()), 'lr': 1e-3},\n",
        "])\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "\n",
        "    # for param in model.bert.encoder.layer[12 - epoch - 1].parameters():\n",
        "    #         param.requires_grad = True\n",
        "\n",
        "    for idx, batch in enumerate(go_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        pred = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = loss_fn(pred, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # if idx % 50 == 0:\n",
        "        #     print(loss.item())\n",
        "\n",
        "    # model.eval()\n",
        "    # all_preds = []\n",
        "    # all_labels = []\n",
        "    # with torch.no_grad():\n",
        "    #     for idx, batch in enumerate(val_dataloader):\n",
        "    #         input_ids = batch['input_ids'].to(device)\n",
        "    #         attention_mask = batch['attention_mask'].to(device)\n",
        "    #         labels = batch['labels']\n",
        "\n",
        "    #         preds = model(input_ids, attention_mask)\n",
        "    #         preds = preds.cpu().tolist()\n",
        "    #         preds = [1 if i > 0.5 else 0 for b in preds for i in b]\n",
        "\n",
        "    #         labels = [j for i in labels for j in i]\n",
        "    #         all_preds.extend(preds)\n",
        "    #         all_labels.extend(labels)\n",
        "\n",
        "    # s = f1_score(all_labels, all_preds, average='macro')\n",
        "    # print(f'f1_score={s}')\n"
      ],
      "metadata": {
        "id": "_VmEQ0G9C5wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "for epoch in range(4):\n",
        "    model.train()\n",
        "\n",
        "    for param in model.bert.encoder.layer[:6].parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        pred = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = loss_fn(pred, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # if idx % 50 == 0:\n",
        "        #     print(loss.item())\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            preds = model(input_ids, attention_mask)\n",
        "            preds = preds.cpu().tolist()\n",
        "            preds = [1 if i > 0.5 else 0 for b in preds for i in b]\n",
        "\n",
        "            labels = [j for i in labels for j in i]\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    s = f1_score(all_labels, all_preds, average='macro')\n",
        "    print(f'f1_score={s}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NONHpdHgPqAj",
        "outputId": "3c359175-5776-426b-eb65-39ca428dab8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1_score=0.8085770681574163\n",
            "f1_score=0.8334599829402474\n",
            "f1_score=0.837209020711146\n",
            "f1_score=0.8190136674988797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yixA4lyU7ULq"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'large1.pth')"
      ],
      "metadata": {
        "id": "WcZBfi8Cf5v_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "098ef079-fe72-44c0-c25f-c006819587cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-151349f3950b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'large1.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmZFZApq8sXA"
      },
      "outputs": [],
      "source": [
        "encoded = test_df['text'].apply(lambda x: tokenizer.encode_plus(\n",
        "    x,\n",
        "    max_length=64,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True\n",
        "))\n",
        "# test_df['text'].apply(clean_text)\n",
        "\n",
        "test_df['input_ids'] = encoded.apply(lambda x: x['input_ids'])\n",
        "test_df['attention_mask'] = encoded.apply(lambda x: x['attention_mask'])\n",
        "\n",
        "test_dataset = TweetsDataset(test_df, emotions2)\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Tr-Kjp7eBR8"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "qk = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, batch in enumerate(test_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels']\n",
        "\n",
        "        # pp = []\n",
        "        # for model in models:\n",
        "        ahah = model(input_ids, attention_mask).squeeze(0)\n",
        "        #     pp.append(temp)\n",
        "        # ahah = torch.mean(torch.stack(pp), dim=0)\n",
        "        preds = (ahah > 0.5)\n",
        "        preds = preds.int()\n",
        "        # print(preds)\n",
        "        # break\n",
        "        bb = test_df.loc[idx, 'id']\n",
        "\n",
        "        qk.append([bb] + preds.tolist())\n",
        "\n",
        "output_df = pd.DataFrame(qk, columns=['id', 'Anger', 'Fear', 'Joy', 'Sadness', 'Surprise'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eKbE84Q77W-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "df3e0cac-83d0-4537-9f56-a7f68bb0cfc3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d859a0f8-7be7-4e48-a8c4-b6371170a320\", \"pred_eng.csv\", 3747)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "output_df.to_csv('pred_eng.csv', index=False)\n",
        "from google.colab import files\n",
        "files.download('pred_eng.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXc5IPuk84ha"
      },
      "outputs": [],
      "source": [
        "output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9Z3dAaBV3aC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}